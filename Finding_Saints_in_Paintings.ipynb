{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Finding Saints in Paintings\n",
    "This work is based on the paper:\\\n",
    "*Milani F, Fraternali P (2021) A Data Set and a Convolutional Model for Iconography Classification in Paintings. J Comput Cult Herit 14:1â€“18. https://doi.org/10.1145/3458885*\\\n",
    "The data set can be found here: http://www.artdl.org/ \\\n",
    "\\\n",
    "The goal of this notebook is to replicate the results of Milani et al but instead of using a CNN model, I will be using the Vision Transformer (ViT).\n",
    "The code is based on these two tutorials on fine-tuning ViT:\\\n",
    "https://colab.research.google.com/github/NielsRogge/Transformers-Tutorials/blob/master/VisionTransformer/Fine_tuning_the_Vision_Transformer_on_CIFAR_10_with_the_%F0%9F%A4%97_Trainer.ipynb#scrollTo=szWwJmqPHZ-r\n",
    "\\\n",
    "https://colab.research.google.com/github/NielsRogge/Transformers-Tutorials/blob/master/VisionTransformer/Fine_tuning_the_Vision_Transformer_on_CIFAR_10_with_PyTorch_Lightning.ipynb#scrollTo=VV8_9IjhKiDh\n",
    "\n",
    "The Class Activation Map (CAM) implementation is based on this:\n",
    "https://github.com/jacobgil/pytorch-grad-cam/blob/master/usage_examples/vit_example.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import libraries\n",
    "(install requirements, if necessary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "import re\n",
    "from zipfile import ZipFile\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.metrics import classification_report, precision_score, recall_score, f1_score, accuracy_score, average_precision_score\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import glob\n",
    "from PIL import Image as PIL_Image\n",
    "import cv2\n",
    "from matplotlib import pyplot as plt\n",
    "import torch\n",
    "from torch.utils.data import DataLoader as TorchLoader\n",
    "import torch.nn as nn\n",
    "import pytorch_lightning as pl\n",
    "from pytorch_lightning import Trainer\n",
    "from pytorch_lightning.callbacks import EarlyStopping\n",
    "from pytorch_lightning.loggers import TensorBoardLogger\n",
    "from torchvision.transforms import ToTensor, RandomHorizontalFlip\n",
    "from datasets import (Dataset, \n",
    "                      load_metric, \n",
    "                      load_dataset, \n",
    "                      Features, \n",
    "                      ClassLabel, \n",
    "                      Array3D,\n",
    "                      Image)\n",
    "from transformers import (ViTModel, \n",
    "                          ViTForImageClassification,\n",
    "                          ViTFeatureExtractor, \n",
    "                          TrainingArguments, \n",
    "                          AdamW) \n",
    "from transformers import (DeiTModel,  \n",
    "                          DeiTFeatureExtractor)\n",
    "from transformers.modeling_outputs import SequenceClassifierOutput\n",
    "from pytorch_grad_cam import GradCAM, \\\n",
    "    ScoreCAM, \\\n",
    "    GradCAMPlusPlus, \\\n",
    "    AblationCAM, \\\n",
    "    XGradCAM, \\\n",
    "    EigenCAM, \\\n",
    "    EigenGradCAM, \\\n",
    "    LayerCAM, \\\n",
    "    FullGrad\n",
    "from pytorch_grad_cam import GuidedBackpropReLUModel\n",
    "from pytorch_grad_cam.utils.image import show_cam_on_image, preprocess_image\n",
    "from pytorch_grad_cam.ablation_layer import AblationLayerVit\n",
    "\n",
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Download data \n",
    "(if not already done)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Dataset is stored in Gdrive of the creators\n",
    "This can take a minute or two. You should get an output like this:\n",
    "\n",
    ">>> Downloading...\n",
    ">>> From: https://drive.google.com/uc?id=16FK1YnHPhGqCHf_EpovzcH0v90yXcCer\n",
    ">>> To: /home/jovyan/DEVKitArtDL.zip\n",
    ">>> 100% 3.62G/3.62G [01:03<00:00, 68.7MB/s]\n",
    "\n",
    "Please copy the zip-file location into the variable in the next cell.\n",
    "'''\n",
    "!gdown https://drive.google.com/uc?id=16FK1YnHPhGqCHf_EpovzcH0v90yXcCer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "zip_file_location = '/home/jovyan/DEVKitArtDL.zip'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chose Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ViT = 'google/vit-base-patch16-224-in21k'\n",
    "DeIT = 'facebook/deit-base-distilled-patch16-224'\n",
    "tiny_ViT = 'lysandre/tiny-vit-random'\n",
    "\n",
    "\n",
    "ViT_extractor = ViTFeatureExtractor.from_pretrained(ViT)\n",
    "#DeIT_extractor = DeiTFeatureExtractor.from_pretrained(DeIT)\n",
    "\n",
    "\n",
    "Vit_model = ViTModel.from_pretrained(ViT)\n",
    "#DeIT_model = DeiTModel.from_pretrained(DeIT)\n",
    "\n",
    "\n",
    "#########################\n",
    "version = ViT\n",
    "extractor = ViT_extractor\n",
    "model = Vit_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataLoader():\n",
    "    '''\n",
    "    Class for loading data and labels as provided by Milani et al 2021\n",
    "\n",
    "    returns:\n",
    "    data (dict): (train:[PIL Images], val:[PIL Images], test:[PIL Images])\n",
    "    labels (dict): (train:[Label IDs], val:[Label IDs], test:[Label IDs])\n",
    "    num_labels (int): number of labels\n",
    "    ID2Label (dict): {label_as_number : label_as_string}\n",
    "    Label2ID (dict): {label_as_string : label_as_number}\n",
    "    '''\n",
    "\n",
    "    def __init__(self, zip_file_location):\n",
    "        self.zip_file_location = zip_file_location\n",
    "        self.info_frame = pd.read_csv(ZipFile(zip_file_location).open('DEVKitArt/info.csv'))\n",
    "        self.drop_ambiguous()\n",
    "        self.num_labels = 20\n",
    "\n",
    "    def drop_ambiguous(self):\n",
    "        '''\n",
    "        Drops all images that have more than one label\n",
    "        '''\n",
    "        self.drop = self.info_frame.loc[(self.info_frame.sum(axis=1) > 1), :]['item']\n",
    "        self.info_frame.drop(self.drop.index, inplace=True)  \n",
    "\n",
    "    def load_labels(self):\n",
    "        '''\n",
    "        Extracts label details from info.csv\n",
    "        '''\n",
    "        self.ID2Label = {id+1:(label if re.search(r'\\(.*?\\)', label) is None else re.search(r'\\(.*?\\)', label)[0][1:-1]) for (id, label) in enumerate(self.info_frame.columns[1:-1])}\n",
    "        self.Label2ID = {(label if re.search(r'\\(.*?\\)', label) is None else re.search(r'\\(.*?\\)', label)[0][1:-1]):id+1 for (id, label) in enumerate(self.info_frame.columns[1:-1])}\n",
    "\n",
    "    def update_frame(self):\n",
    "        '''\n",
    "        Adds information about labels to info.csv data frame\n",
    "        '''\n",
    "        self.info_frame[\"label_name\"] = [self.ID2Label[id] for id in self.IDs]\n",
    "        self.info_frame[\"label_id\"] = self.IDs \n",
    "\n",
    "    def add_None_Label(self):\n",
    "        '''\n",
    "        Adds a None label to images that do not depict any saint\n",
    "        '''\n",
    "        self.ID2Label[0] = \"None\"\n",
    "        self.Label2ID[\"None\"] = 0\n",
    "        array = self.info_frame.iloc[:,1:-1].to_numpy()\n",
    "        array = np.insert(array, 0, np.zeros(array.shape[0]), axis=1)\n",
    "        self.IDs = np.argmax(array, axis=1)\n",
    "        \n",
    "    def shrink(self, frac:float = 1):\n",
    "        '''\n",
    "        Since there are far more Mary and None labels than any other, this function\n",
    "        allows to shrink the examples for those two labels to a smaller fraction\n",
    "        '''\n",
    "        self.info_frame = pd.concat([\n",
    "                   self.info_frame[self.info_frame.label_id > 1],\n",
    "                   self.info_frame[self.info_frame.label_id < 2].sample(frac=frac, replace=False, random_state=1)]\n",
    "                ).sample(frac = 1)\n",
    "        \n",
    "    def drop_by_occurrence(self, drop:int = 10):\n",
    "        '''\n",
    "        Drops n examples based on the least label frequency.\n",
    "        '''\n",
    "        occurrences = self.info_frame.groupby('label_name').count().item.to_list()\n",
    "        occurrences.sort()\n",
    "        self.info_frame = self.info_frame.groupby('label_name').filter(lambda x: len(x) > occurrences[drop]-1)\n",
    "        self.num_labels -= drop\n",
    "\n",
    "    def split(self, resize_factor:tuple = (32,32) ):\n",
    "        '''\n",
    "        Splits data set into train, val, and test set.\n",
    "        '''\n",
    "        self.stats = {lab:0 for lab in self.Label2ID.keys()}\n",
    "        zf = ZipFile(self.zip_file_location)\n",
    "        self.data = {}\n",
    "        self.labels = {}\n",
    "        for split in self.info_frame[\"set\"].unique():\n",
    "            self.data[split] = []\n",
    "            self.labels[split] = []\n",
    "            for row in tqdm(self.info_frame[self.info_frame[\"set\"] == split].itertuples(index=False, name=None), \n",
    "                          desc=split, \n",
    "                          total=self.info_frame[self.info_frame[\"set\"] == split].shape[0]):\n",
    "                img = zf.open('DEVKitArt/JPEGImages/' + row[0] + '.jpg') \n",
    "                img = PIL_Image.open(img)\n",
    "                self.data[split].append(img.resize(resize_factor))\n",
    "                self.labels[split].append(row[-1])\n",
    "                self.stats[self.ID2Label[row[-1]]] += 1\n",
    "    \n",
    "    def refresh(self):\n",
    "        '''\n",
    "        Adjusts label and ids of dict and dataframe to the dropped labels\n",
    "        '''\n",
    "        id_is_key = {}\n",
    "        label_is_key = {}\n",
    "        for idx, label_name in enumerate(self.info_frame['label_name'].unique()):\n",
    "            self.info_frame.loc[self.info_frame['label_name'] == label_name, 'label_id'] = idx\n",
    "            id_is_key[idx] = label_name\n",
    "            label_is_key[label_name] = idx\n",
    "        self.ID2Label = id_is_key\n",
    "        self.Label2ID = label_is_key\n",
    "        \n",
    "\n",
    "    def augmentation(self):\n",
    "        '''\n",
    "        Performs horizontal flip on images that are less frequent (= that are neither NONE nor MARY).\n",
    "        '''\n",
    "        aug_data = []\n",
    "        aug_label = []\n",
    "        args_least_freq = np.argwhere((np.array(self.labels['train']) != self.Label2ID['MARY']) & \n",
    "                          (np.array(self.labels['train']) != self.Label2ID['None']))\n",
    "        for idx in tqdm(args_least_freq, \n",
    "                        desc='Augmentation',\n",
    "                       total= args_least_freq.shape[0]):\n",
    "            aug_data.append(RandomHorizontalFlip(1)(self.data['train'][int(idx)]))\n",
    "            aug_label.append(self.labels['train'][int(idx)])\n",
    "            self.stats[self.ID2Label[self.labels['train'][int(idx)]]] += 1\n",
    "            \n",
    "        self.data['train'] = self.data['train'] + aug_data\n",
    "        self.labels['train'] = self.labels['train'] + aug_label\n",
    "        \n",
    "    def load(self, frac:float = 1, drop:int = 9, augment = False, resize_factor:tuple = (128,128)):\n",
    "        '''\n",
    "        Calls relevant functions in correct order.\n",
    "        '''\n",
    "        self.load_labels()\n",
    "        self.add_None_Label()\n",
    "        self.update_frame()\n",
    "        self.shrink(frac)\n",
    "        self.drop_by_occurrence(drop)\n",
    "        self.refresh()\n",
    "        self.split(resize_factor)\n",
    "        if augment:\n",
    "            self.augmentation()\n",
    "        self.get_stats()\n",
    "        return self.data, self.labels, self.num_labels, self.ID2Label, self.Label2ID\n",
    "\n",
    "    def demo(self, resize_factor:int = 3):\n",
    "        '''\n",
    "        Displays random image including label as string and int\n",
    "        '''\n",
    "        rand = random.randint(0, len(self.data['train']))\n",
    "        img = self.data['train'][rand]\n",
    "        label = self.labels['train'][rand]\n",
    "        width, height = img.size\n",
    "        width, height = int(width * resize_factor), int(height * resize_factor)\n",
    "        img = img.resize((width, height))\n",
    "        img.show()\n",
    "        display(img)\n",
    "        print(self.ID2Label[label], label)\n",
    "    \n",
    "    def get_stats(self):\n",
    "        return pd.DataFrame.from_dict(self.stats,orient='index', columns = ['Frequency']).sort_values(by=['Frequency'], ascending=False)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "frac = 1\n",
    "drop = 9\n",
    "augment = False\n",
    "resize_factor = (224,224)\n",
    "dl = DataLoader(zip_file_location)\n",
    "data, labels, num_labels, ID2Label, Label2ID = dl.load(frac=frac, \n",
    "                                                       drop=drop, \n",
    "                                                       augment=augment, \n",
    "                                                       resize_factor=resize_factor)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Some insights on the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dl.get_stats()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Displays a random image including its labels\n",
    "# You can add a resize factor\n",
    "dl.demo(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_distribution(ids:dict, id2label, split:str = 'train', save = False, save_as = '_Distribution'):\n",
    "    df = pd.DataFrame.from_dict({'Label' : [id2label[id] for id in ids[split]]})\n",
    "    plt.figure(figsize=(10,6))\n",
    "    plt.title(split)\n",
    "    ax = sns.countplot(y = 'Label', \n",
    "                 data = df, \n",
    "                 order = df['Label'].value_counts().index, \n",
    "                 palette = 'Set3')\n",
    "    if save:\n",
    "        fig = ax.get_figure()\n",
    "        fig.savefig(split + save_as + '.png', bbox_inches=\"tight\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "plot_distribution(labels, ID2Label, 'train', save=True, save_as ='_Distribution')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "plot_distribution(labels, ID2Label, 'val')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "plot_distribution(labels, ID2Label, 'test')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocess\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PreProcess():\n",
    "    '''\n",
    "    Loads data and labels to Huggingface's Dataset class and performs resizing/formatting\n",
    "    '''\n",
    "    def __init__(self, data, labels, ID2Label, extractor):\n",
    "        self.data = data\n",
    "        self.labels = labels\n",
    "        self.label_names = [ID2Label[ID] for ID in range(len(ID2Label))]\n",
    "        self.feature_extractor = extractor\n",
    "        self.features = Features({\n",
    "                    'label': ClassLabel(\n",
    "                        names=self.label_names),\n",
    "                    'img': Image(),\n",
    "                    'pixel_values': Array3D(dtype=\"float32\", shape=(3, 224, 224)),\n",
    "                })\n",
    "        self.to_tensor = ToTensor()\n",
    "        \n",
    "    def split(self):\n",
    "        '''\n",
    "        Splits data in training, validation, and test set as huggingface's dataset object\n",
    "        '''\n",
    "        print('Load train set as dataset object...')\n",
    "        self.train_ds = {'img' : self.data['train'], 'label' : self.labels['train']}\n",
    "        self.train_ds = Dataset.from_dict(self.train_ds)\n",
    "\n",
    "        print('Load validation set as dataset object...')\n",
    "        self.val_ds = {'img' : self.data['val'], 'label' : self.labels['val']}\n",
    "        self.val_ds = Dataset.from_dict(self.val_ds)\n",
    "\n",
    "        print('Load test set as dataset object...\\n')\n",
    "        self.test_ds = {'img' : self.data['test'], 'label' : self.labels['test']}\n",
    "        self.test_ds = Dataset.from_dict(self.test_ds)\n",
    "\n",
    "    def format_(self):\n",
    "        '''\n",
    "        Brings data in right format for Vision Transformer.\n",
    "        '''\n",
    "        def preprocess_images(examples):\n",
    "            images = examples['img']\n",
    "            images = [self.to_tensor(image) for image in images]\n",
    "            inputs = self.feature_extractor(images=images)\n",
    "            examples['pixel_values'] = inputs['pixel_values']\n",
    "            return examples\n",
    "        \n",
    "        print('Bring train data in right format for Vision Transformer...')\n",
    "        self.preprocessed_train_ds = self.train_ds.map(preprocess_images, batched=True, features=self.features)\n",
    "        print('Bring validation data in right format for Vision Transformer...')\n",
    "        self.preprocessed_val_ds = self.val_ds.map(preprocess_images, batched=True, features=self.features)\n",
    "        print('Bring test data in right format for Vision Transformer...')\n",
    "        self.preprocessed_test_ds = self.test_ds.map(preprocess_images, batched=True, features=self.features)\n",
    "        \n",
    "    def loader(self, train_batch_size = 2, eval_batch_size = 2):\n",
    "        '''\n",
    "        Creates Dataloader for training on batches.\n",
    "        '''\n",
    "        def collate_fn(examples):\n",
    "            pixel_values = torch.stack([torch.tensor(example[\"pixel_values\"]) for example in examples])\n",
    "            labels = torch.tensor([example[\"label\"] for example in examples])\n",
    "            return {\"pixel_values\": pixel_values, \"labels\": labels}\n",
    "        \n",
    "        self.train_dataloader = TorchLoader(self.preprocessed_train_ds, shuffle=True, collate_fn=collate_fn, batch_size=train_batch_size)\n",
    "        self.val_dataloader = TorchLoader(self.preprocessed_val_ds, collate_fn=collate_fn, batch_size=eval_batch_size)\n",
    "        self.test_dataloader = TorchLoader(self.preprocessed_test_ds, collate_fn=collate_fn, batch_size=eval_batch_size)\n",
    "\n",
    "    def process(self,train_batch_size = 2, eval_batch_size = 2):\n",
    "        '''\n",
    "        Calls relevant functions in correct order.\n",
    "        '''\n",
    "        self.split()\n",
    "        self.format_()\n",
    "        self.loader(train_batch_size, eval_batch_size)\n",
    "        return self.train_dataloader, self.val_dataloader, self.test_dataloader\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_batch_size, eval_batch_size = 2, 2\n",
    "pp = PreProcess(data, labels, ID2Label, extractor)\n",
    "train_dataloader, val_dataloader, test_dataloader = pp.process(train_batch_size, eval_batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Vision Transformer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ViT(pl.LightningModule):\n",
    "    '''\n",
    "    Class for Vision Transformer.\n",
    "    '''\n",
    "    def __init__(self, \n",
    "                 version, \n",
    "                 ID2Label, \n",
    "                 Label2ID, \n",
    "                 num_labels,\n",
    "                 lr = 5e-5):\n",
    "        super(ViT, self).__init__()\n",
    "        self.vit = ViTForImageClassification.from_pretrained(version,\n",
    "                                                              num_labels=num_labels,\n",
    "                                                              id2label=ID2Label,\n",
    "                                                              label2id=Label2ID)\n",
    "        self.lr = lr\n",
    "\n",
    "    def forward(self, pixel_values):\n",
    "        '''\n",
    "        Forward pass.\n",
    "        '''\n",
    "        outputs = self.vit(pixel_values=pixel_values)\n",
    "        return outputs.logits\n",
    "        \n",
    "    def common_step(self, batch, batch_idx):\n",
    "        '''\n",
    "        Unpacks batch and computes loss and accuracy.\n",
    "        '''\n",
    "        pixel_values = batch['pixel_values']\n",
    "        labels = batch['labels']\n",
    "        logits = self(pixel_values)\n",
    "\n",
    "        criterion = nn.CrossEntropyLoss()\n",
    "        loss = criterion(logits, labels)\n",
    "        predictions = logits.argmax(-1)\n",
    "        correct = (predictions == labels).sum().item()\n",
    "        accuracy = correct/pixel_values.shape[0]\n",
    "        return loss, accuracy\n",
    "      \n",
    "    def training_step(self, batch, batch_idx):\n",
    "        loss, accuracy = self.common_step(batch, batch_idx)     \n",
    "        self.log(\"training_loss\", loss)\n",
    "        self.log(\"training_accuracy\", accuracy)\n",
    "        return loss\n",
    "    \n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        loss, accuracy = self.common_step(batch, batch_idx)     \n",
    "        self.log(\"validation_loss\", loss, on_epoch=True)\n",
    "        self.log(\"validation_accuracy\", accuracy, on_epoch=True)\n",
    "        return loss\n",
    "\n",
    "    def test_step(self, batch, batch_idx):\n",
    "        loss, accuracy = self.common_step(batch, batch_idx)     \n",
    "        return loss\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        return AdamW(self.parameters(), lr=self.lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Train_VisionTransformer():\n",
    "    '''\n",
    "    Trainer for Vision Transformer.\n",
    "    '''\n",
    "    def __init__(self, \n",
    "                 version, \n",
    "                 train_dataloader, \n",
    "                 val_dataloader, \n",
    "                 test_dataloader,\n",
    "                 ID2Label, \n",
    "                 Label2ID, \n",
    "                 num_labels,\n",
    "                 lr = 5e-5):\n",
    "        self.early_stop_callback = EarlyStopping(\n",
    "                                    monitor='val_loss',\n",
    "                                    patience=3,\n",
    "                                    strict=False,\n",
    "                                    verbose=False,\n",
    "                                    mode='min'\n",
    "                                )\n",
    "        self.model = ViT(version, \n",
    "                         ID2Label, \n",
    "                         Label2ID, \n",
    "                         num_labels,\n",
    "                         lr)\n",
    "        self.target_names = [ID2Label[ID] for ID in range(len(ID2Label))]\n",
    "        self.train_dataloader = train_dataloader\n",
    "        self.val_dataloader = val_dataloader\n",
    "        self.test_dataloader = test_dataloader\n",
    "        self.trainer = Trainer(gpus=1, callbacks=[EarlyStopping(monitor='validation_loss')])\n",
    "        \n",
    "    def train(self):\n",
    "        '''\n",
    "        Fine-tunes the model to the new data.\n",
    "        '''\n",
    "        self.trainer.fit(model = self.model, \n",
    "                         train_dataloaders = self.train_dataloader, \n",
    "                         val_dataloaders = self.val_dataloader)\n",
    "    \n",
    "    \n",
    "    def test(self):\n",
    "        '''\n",
    "        Tests the fine-tuned model on the test data and computes precision, recall, f1, and accuracy.\n",
    "        '''\n",
    "        self.target = []\n",
    "        self.pred = []\n",
    "        self.model.eval()\n",
    "        for batch in tqdm(self.test_dataloader, desc = 'Test',total=len(self.test_dataloader)):\n",
    "            for label, example in zip(batch['labels'], batch['pixel_values']):\n",
    "                self.target.append(label)\n",
    "                self.pred.append(int(self.model(example.reshape((1, 3, 224, 224))).argmax(-1)))\n",
    "        self.pred = np.array(self.pred)\n",
    "        self.target = np.array(self.target)\n",
    "        print(classification_report(self.target, self.pred, target_names=self.target_names))\n",
    "        \n",
    "                          \n",
    "    def confusion_matrix(self, save=False, name = 'ViT'):\n",
    "        '''\n",
    "        Plots confusion matrix of the results on the test data.\n",
    "        '''\n",
    "        cm = confusion_matrix(self.target, self.pred)\n",
    "        eps = 0.000000001\n",
    "        cm = cm/(np.sum(cm, axis=1)+eps)\n",
    "        plt.figure(figsize=(10,6))\n",
    "        ax = sns.heatmap(cm, annot=True, fmt='.3f',cmap='Blues')\n",
    "        ax.set_title('Confusion Matrix (Recall)')\n",
    "        ax.set_xlabel('\\nPrediction')\n",
    "        ax.set_ylabel('Groundtruth ')\n",
    "        ax.xaxis.set_ticklabels(self.target_names)\n",
    "        ax.yaxis.set_ticklabels(self.target_names)\n",
    "        ax.set_xticklabels(ax.get_xticklabels(), rotation=90)\n",
    "        ax.set_yticklabels(ax.get_yticklabels(), rotation=360)\n",
    "        plt.show()\n",
    "        if save:\n",
    "            fig = ax.get_figure()\n",
    "            fig.savefig('ConfusionMatrix/' + name + '.png')\n",
    "        \n",
    "    def save(self, name):\n",
    "        '''\n",
    "        Saves the model weights as statedict.\n",
    "        '''\n",
    "        torch.save(self.model.state_dict(), '/home/jovyan/Model_Checkpoints/' + name + '_stateDict.pt')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 5e-5\n",
    "train_vit = Train_VisionTransformer(version,  \n",
    "                                    train_dataloader, \n",
    "                                    val_dataloader, \n",
    "                                    test_dataloader,\n",
    "                                    ID2Label, \n",
    "                                    Label2ID, \n",
    "                                    num_labels = num_labels,\n",
    "                                    lr = learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_vit.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_vit.test()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "train_vit.confusion_matrix(save=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_vit.save('ViT_v3')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hyper-Parameter Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TUNE():\n",
    "    '''\n",
    "    Class for hyper-parameter tuning.\n",
    "    '''\n",
    "    def __init__(self, lr, batch):\n",
    "        self.lr = lr\n",
    "        self.batch = batch\n",
    "        self.log = {\n",
    "            'model_name' : [],\n",
    "            'learning_rate' : [],\n",
    "            'batch_size' : [],\n",
    "            'Precision(macro)' : [],\n",
    "            'Precision(micro)' : [],\n",
    "            'Recall(macro)' : [],\n",
    "            'Recall(micro)' : [],\n",
    "            'F1(macro)' : [],\n",
    "            'F1(micro)' : [],\n",
    "            'Accuracy' : []\n",
    "        }\n",
    "    \n",
    "    def get_data(self,\n",
    "                  preprocessed_train_ds,\n",
    "                  preprocessed_val_ds,\n",
    "                  preprocessed_test_ds):\n",
    "        '''\n",
    "        Retrieves datasets.\n",
    "        '''\n",
    "        self.preprocessed_train_ds = preprocessed_train_ds\n",
    "        self.preprocessed_val_ds = preprocessed_val_ds\n",
    "        self.preprocessed_test_ds = preprocessed_test_ds\n",
    "    \n",
    "    def get_model_details(self,\n",
    "                         version, \n",
    "                         ID2Label, \n",
    "                         Label2ID, \n",
    "                         num_labels):\n",
    "        '''\n",
    "        Retrieves model details.\n",
    "        '''\n",
    "        self.version = version\n",
    "        self.ID2Label = ID2Label\n",
    "        self.Label2ID = Label2ID\n",
    "        self.num_labels = num_labels\n",
    "        \n",
    "    def get_batch(self, train_batch_size, eval_batch_size):\n",
    "        '''\n",
    "        Creates batch with varying sizes.\n",
    "        '''\n",
    "        def collate_fn(examples):\n",
    "            pixel_values = torch.stack([torch.tensor(example[\"pixel_values\"]) for example in examples])\n",
    "            labels = torch.tensor([example[\"label\"] for example in examples])\n",
    "            return {\"pixel_values\": pixel_values, \"labels\": labels}\n",
    "        \n",
    "        train_dataloader = TorchLoader(self.preprocessed_train_ds, shuffle=True, collate_fn=collate_fn, batch_size=train_batch_size)\n",
    "        val_dataloader = TorchLoader(self.preprocessed_val_ds, collate_fn=collate_fn, batch_size=eval_batch_size)\n",
    "        test_dataloader = TorchLoader(self.preprocessed_test_ds, collate_fn=collate_fn, batch_size=eval_batch_size)\n",
    "        return train_dataloader, val_dataloader, test_dataloader\n",
    "    \n",
    "    def metrics(self, y_pred, y_true):\n",
    "        '''\n",
    "        Computes metrics for each model during tuning.\n",
    "        '''\n",
    "        self.log['Precision(macro)'].append(precision_score(y_true, y_pred, average='macro'))\n",
    "        self.log['Precision(micro)'].append(precision_score(y_true, y_pred, average='micro'))\n",
    "        self.log['Recall(macro)'].append(recall_score(y_true, y_pred, average='macro'))\n",
    "        self.log['Recall(micro)'].append(recall_score(y_true, y_pred, average='micro'))\n",
    "        self.log['F1(macro)'].append(f1_score(y_true, y_pred, average='macro'))\n",
    "        self.log['F1(micro)'].append(f1_score(y_true, y_pred, average='micro'))\n",
    "        self.log['Accuracy'].append(accuracy_score(y_true, y_pred))\n",
    "\n",
    "    def tune(self):\n",
    "        '''\n",
    "        Trains a model for each learning rate and batch size.\n",
    "        '''\n",
    "        idx = 1 \n",
    "        skip_first = True\n",
    "        for learning_rate in self.lr:\n",
    "            for batch_size in self.batch:\n",
    "                print(f'Train Model: Learning rate: {learning_rate}, Batch size: {batch_size}...\\n')\n",
    "                train_dataloader, val_dataloader, test_dataloader = self.get_batch(batch_size, batch_size)\n",
    "                model = Train_VisionTransformer(\n",
    "                                        self.version,\n",
    "                                        train_dataloader, \n",
    "                                        val_dataloader, \n",
    "                                        test_dataloader,\n",
    "                                        self.ID2Label, \n",
    "                                        self.Label2ID, \n",
    "                                        self.num_labels,\n",
    "                                        learning_rate\n",
    "                )\n",
    "                model.train()\n",
    "                name = 'ViT_' + str(idx)\n",
    "                model.save(name)\n",
    "                model.test()\n",
    "                idx += 1\n",
    "                self.log['model_name'].append(name)\n",
    "                self.log['learning_rate'].append(learning_rate)\n",
    "                self.log['batch_size'].append(batch_size)\n",
    "                pred = model.pred\n",
    "                target = model.target\n",
    "                self.metrics(pred, target)\n",
    "                self.results = pd.DataFrame.from_dict(self.log)\n",
    "                self.results.to_csv('ViT_HypTune.csv', index = False)\n",
    "                model.confusion_matrix(save = True, name = name)\n",
    "                \n",
    "    def get_results(self):\n",
    "        display(self.results)\n",
    "        return self.results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = [0.00003, 0.00002, 0.000005]\n",
    "batch = [2, 8, 16, 64]\n",
    "tune = TUNE(lr, batch)\n",
    "tune.get_data(\n",
    "                    pp.preprocessed_train_ds,\n",
    "                    pp.preprocessed_val_ds,\n",
    "                    pp.preprocessed_test_ds\n",
    ")\n",
    "tune.get_model_details(\n",
    "                    version,              \n",
    "                    ID2Label, \n",
    "                    Label2ID, \n",
    "                    num_labels\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tune.tune()\n",
    "tune.get_results()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def highlight_max(data, color='yellow'):\n",
    "    '''\n",
    "    highlight the maximum\n",
    "    '''\n",
    "    if data.name in ['model_name', 'learning_rate', 'batch_size']:\n",
    "        return ['' for _ in data]\n",
    "    else:\n",
    "        attr = 'background-color: {}'.format(color)\n",
    "        is_max = data == data.max()\n",
    "        return [attr if v else '' for v in is_max]\n",
    "    \n",
    "df = pd.read_csv('ViT_HypTune_reduced_Dataset.csv')\n",
    "df.style.apply(highlight_max)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Model\n",
    "(In case you already saved a fine-tuned version)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Load_ViT():\n",
    "    def __init__(self, name, version, ID2Label, Label2ID, num_labels):\n",
    "        self.FILE = '/home/jovyan/Model_Checkpoints/' + name + '_stateDict.pt'\n",
    "        self.loaded_model = ViT(version, ID2Label, Label2ID, num_labels, 5e-5)\n",
    "        self.loaded_model.load_state_dict(torch.load(self.FILE))\n",
    "        \n",
    "    def get(self):\n",
    "        return self.loaded_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "name = 'ViT_tune_corrected_6'\n",
    "vit = Load_ViT(name, 'google/vit-base-patch16-224-in21k', ID2Label, Label2ID, num_labels).get()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Class Activation Map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CAM():\n",
    "    '''\n",
    "    Class Activation Map for given image and model.\n",
    "    '''\n",
    "    def __init__(self, model, info_frame, zip_file_location, ID2Label):\n",
    "        self.model = model\n",
    "        self.model.eval()\n",
    "        self.info_frame = info_frame[info_frame['set'] == 'test']\n",
    "        self.zf = ZipFile(zip_file_location)\n",
    "        self.ID2Label = ID2Label\n",
    "        self.target_layers = [self.model.vit.vit.encoder.layer[11].layernorm_before]\n",
    "        self.methods = \\\n",
    "        {\"gradcam\": GradCAM,\n",
    "         \"scorecam\": ScoreCAM,\n",
    "         \"gradcam++\": GradCAMPlusPlus,\n",
    "         \"xgradcam\": XGradCAM,\n",
    "         \"eigencam\": EigenCAM,\n",
    "         \"eigengradcam\": EigenGradCAM,\n",
    "         \"layercam\": LayerCAM\n",
    "         }\n",
    "        self.targets = None\n",
    "        \n",
    "    def reshape_transform(self, tensor, height=14, width=14):\n",
    "        result = tensor[:, 1:, :].reshape(tensor.size(0),height, width, tensor.size(2))\n",
    "        result = result.transpose(2, 3).transpose(1, 2)\n",
    "        return result\n",
    "\n",
    "    def remind_methods(self):\n",
    "        '''\n",
    "        Returns all possible CAM methods.\n",
    "        '''\n",
    "        print(self.methods.keys())\n",
    "        return list(self.methods.keys())\n",
    "        \n",
    "    def remind_labels(self):\n",
    "        '''\n",
    "        Returns all class names.\n",
    "        '''\n",
    "        print(self.info_frame['label_name'].unique())\n",
    "        return self.info_frame['label_name'].unique()\n",
    "        \n",
    "    def chose_method(self, method):\n",
    "        '''\n",
    "        Selects CAM method.\n",
    "        '''\n",
    "        \n",
    "        self.cam = self.methods[method](model=self.model,\n",
    "                   target_layers=self.target_layers,\n",
    "                   use_cuda=torch.cuda.is_available(),\n",
    "                   reshape_transform=self.reshape_transform)\n",
    "        self.cam.batch_size = 32\n",
    "        self.single_method = method\n",
    "        \n",
    "    def fit_all(self):\n",
    "        '''\n",
    "        Applies all CAM methods to given image.\n",
    "        '''\n",
    "        self.cams = {}\n",
    "        for name, method in self.methods.items():\n",
    "            self.cams[name] = self.methods[name](\n",
    "                model=self.model,\n",
    "                target_layers=self.target_layers,\n",
    "                use_cuda=torch.cuda.is_available(),\n",
    "                reshape_transform=self.reshape_transform)\n",
    "            self.cams[name].batch_size = 32\n",
    "        \n",
    "    def chose_image(self, label = 'MARY'):\n",
    "        '''\n",
    "        Selects random image from given class.\n",
    "        '''\n",
    "        name = self.info_frame[self.info_frame['label_name'] == label].sample().iloc[0,0]\n",
    "        img = self.zf.read('DEVKitArt/JPEGImages/' + name + '.jpg')\n",
    "        self.rgb_img = cv2.imdecode(np.frombuffer(img, np.uint8), 1)[:, :, ::-1]  \n",
    "        \n",
    "        self.rgb_img = cv2.resize(self.rgb_img, (224, 224))\n",
    "        self.rgb_img = np.float32(self.rgb_img) / 255\n",
    "        self.input_tensor = preprocess_image(self.rgb_img, mean=[0.5, 0.5, 0.5],\n",
    "                                    std=[0.5, 0.5, 0.5])\n",
    "        \n",
    "    def predict(self, label= 'MARY'):\n",
    "        '''\n",
    "        Model output on given image.\n",
    "        '''\n",
    "        title = 'True: ' + label + ', Pred: '\n",
    "        title += self.ID2Label[int(self.model(self.input_tensor.to(device='cuda')).argmax(-1))]\n",
    "        return title\n",
    "        \n",
    "    def show_all(self, label = 'MARY', save=False, save_as='CAM_01'):\n",
    "        '''\n",
    "        Displays results from fit_all()\n",
    "        '''\n",
    "        self.fit_all()\n",
    "        self.chose_image(label)\n",
    "        imgs = {self.predict(label) : self.rgb_img}\n",
    "        for name, cam in self.cams.items():\n",
    "            grayscale_cam = cam(\n",
    "                input_tensor=self.input_tensor,\n",
    "                targets=self.targets,\n",
    "                eigen_smooth=True,\n",
    "                aug_smooth=True)\n",
    "            grayscale_cam = grayscale_cam[0, :]\n",
    "            imgs[name] = show_cam_on_image(self.rgb_img, grayscale_cam)\n",
    "        fig, axs = plt.subplots(2,4, figsize=(18,7))\n",
    "        for ax, title_img in zip(axs.ravel(), imgs.items()):\n",
    "            ax.set_xticks([])\n",
    "            ax.set_yticks([])\n",
    "            ax.set_title(title_img[0])\n",
    "            ax.imshow(title_img[1])\n",
    "        if save:\n",
    "            fig.savefig('CAM/' + save_as + '.png')\n",
    "        \n",
    "    def show_single(self, label = 'MARY', save=False, name='CAM_01'):\n",
    "        '''\n",
    "        Displays image and CAM.\n",
    "        '''\n",
    "        self.chose_image(label)\n",
    "        grayscale_cam = self.cam(input_tensor=self.input_tensor,\n",
    "                        targets=self.targets,\n",
    "                        eigen_smooth=True,\n",
    "                        aug_smooth=True)\n",
    "        grayscale_cam = grayscale_cam[0, :]\n",
    "        self.cam_image = show_cam_on_image(self.rgb_img, grayscale_cam)\n",
    "        self.cam_image = cv2.cvtColor(self.cam_image, cv2.COLOR_BGR2RGB)\n",
    "        title = 'Class Activation Map ' + '(' + self.single_method + ')'\n",
    "        fig, axs = plt.subplots(1,2, figsize=(14,12))\n",
    "        for ax in axs:\n",
    "            ax.set_xticks([])\n",
    "            ax.set_yticks([])\n",
    "        axs[1].set_title(title)\n",
    "        axs[0].set_title(self.predict(label))\n",
    "        axs[0].imshow(self.rgb_img)\n",
    "        axs[1].imshow(self.cam_image)\n",
    "        if save:\n",
    "            fig.savefig('CAM/' + name + '.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cam = CAM(vit, dl.info_frame, zip_file_location, ID2Label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "method = 'gradcam'\n",
    "cam.chose_method(method)\n",
    "cam.show_single(label='JEROME', save=True, name='JEROME_03')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for label in cam.remind_labels():\n",
    "    for idx in range(1, 4):\n",
    "        if label in ['None']:\n",
    "            continue\n",
    "        name = label + '_ViT_tune_9_' + str(idx)\n",
    "        cam.show_all(label=label, save=True, save_as=name)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
